---
title: "Data Analysis Project - How to Increase your Overwatch Skill Rating"
author: "Brian Deignan (deignan3) & Kai Chang (kaihc2)"
date: 'July 29, 2019'
output:
  html_document: 
    toc: yes
    fig_width: 10
  pdf_document: default
urlcolor: cyan
---

![Overwatch heroes Lucio, Wrecking Ball and Soldier 76 in action](ow6.jpg)

***

## Explaining the Overwatch Skill Rating

### Introduction
Our dataset consists of information on competitive gamers who play the video game Overwatch on Playstation 4. [Overwatch](https://en.wikipedia.org/wiki/Overwatch_(video_game)) is a team-based multiplayer first-person shooter developed and published by Blizzard Entertainment. Overwatch assigns players into two teams of six, with each player selecting from a roster of 30 characters, known as "heroes", each with a unique style of play whose roles are divided into three general categories that fit their role. Players on a team work together to secure and defend control points on a map or escort a payload across the map in a limited amount of time. 

Overwatch has a large community and E-Sports presence online. Players' skill in competitive games is calculated by a "secret" formula at Blizzard that leads to a "skill rating", or "SR" for short. SR ranges from 0 to 5,000, the higher the score the better the player.

Among the community, SR is divided into categories depending on how high the rating is, ranging from Bronze to Grandmaster:

- 500-1499 Bronze
- 1500-1999 Silver
- 2000-2499 Gold
- 2500-2999 Platinum
- 3000-3499 Diamond
- 3500-3999 Master
- 4000-5000 Grandmaster

##### Gameplay

Overwatch pairs two teams up against each other in a match, or game. Each team has six members who must each choose a unique character, called a "hero". More on that later. The goal is to damage the other team and help keep your team protected. In addition to damaging the other team's players, the team has an "objective" to complete, which varies from map to map. A "map" here refers to a setting where a match takes place. The objective in Overwatch typically depends on members from one team being located in a specific area, refered to as the objective, while the other team isn't in that same area. While in the objective area, your team accrues time spent on the objective. If your team accrues enough time, you win the match. It's a little more complicated than this but that's essentially it. 

Holding down an objecitve area is where damaging your opponent comes in to play. When the other team tries to get on the objective at the same time as your team, you deal them damage to keep them away. Hopefully do enough that the opponent "dies". But don't worry, Overwatch is what's called a "respawn" game. After a player "dies" from taking too much damage, they must wait a few seconds and then they rejoin the game and try again to take the objective from the other team

##### Data

We scraped a snapshot of PS4 players' SR (it changes from game to game) whose profiles were public on overwatchtracker.com. We then scraped players' career statistics from the games that they've played from the open source API ovrstat.com which returns convenient JSON formatted data.

Currently we have over two thousand player skill ratings and over two thousand predictor variables. However, our research question will allow us to tailor our question to a small subset of the predictors, around 60 or so.

##### Research Question

Overall we're interested in the question: if a player wants to improve their SR, what should they focus on? Should they try to eliminate more opponents? Heal their teammates? Or, play a certain character? Answers like these will be provided by a predictive model of SR using career player statistics as predictors. The answers we find will allow any player to most efficiently improve their SR and begin climbing their way to Grandmaster!

The answers we find could be used by amateurs and pro Overwatch gamers alike. We think of our analysis as the start of something like ["Moneyball"](https://en.wikipedia.org/wiki/Moneyball) for Overwatch.

### Methods
```{r, echo=FALSE, message=FALSE}
library(tidyverse)
library(lmtest)
```

```{r, message=FALSE}
df = read_csv('data/clean-data.csv')
```

Here we releved our factor variable to use the character `Reinhardt` as the reference level. Reinhardt is the most popular hero used in the game and would be a good baseline to use for comparison.

```{r, echo=FALSE, message=FALSE}
# relevel top hero - brian
df = df %>% 
  select(-c(game.gamesPlayed, game.timePlayed, rank)) %>% 
  mutate(top_hero = relevel(top_hero %>% as.factor(), ref='reinhardt'))
```

Let's explore the data a bit more...

```{r}
df %>% glimpse()
df %>% summary()
```


```{r, echo=FALSE, cache=TRUE}
mean_sr = mean(df$skill_rating)
sd_sr = sd(df$skill_rating)

hist(df$skill_rating,
     prob = TRUE,
     xlab   = "Skill Rating",
     main   = "Histogram of Skill Rating (Playstation 4 players)",
     breaks = 20,
     col    = "#56B4E9",
     border = "#999999")
curve(dnorm(x, mean = mean_sr, sd = sd_sr), 
      col = "#E69F00", add = TRUE, lwd = 3)
```

Above is a normalized (to sum to 1) histogram of the reponse we want to model `skill_rating`. It appears that `skill_rating` (blue bars) looks a lot like a normal distribution (the orange line). This is a good thing, as this makes it easier to adhere to the assumptions of linear regression model we will use to explain the variation in player skill.

##### Most played heroes

At the time of the analysis, there are 30 heroes in Overwatch to choose from. Each hero has a unique play style. There are three types of heroes which determine the play style: Tank, Damage, or Support. Tank heroes can take a lot of damage and typically protect their teammates while trying to remain on the objective area. Damage heroes have the ability to do the most damage to the opposition. They have lower health than tank heroes, meaning they can "die" more easily. Lastly, there are support heroes. These heroes typically have the least health and do the least amount of damage to opponents. Instead of holding down an objective or damaging opponents, Support heroes main job to restore the health of their teammates, which is their primary special role on the team. That being said, all heroes can do damage and a really good Support player can beat a hard-to-kill tank in the game. You can read more about the heroes [here](https://en.wikipedia.org/wiki/Characters_of_Overwatch#Characters).

First, let's see which heroes are the most popular to play, and which type of hero they are.

```{r, echo=FALSE, cache=TRUE}
# - bar chart of top players - brian
df %>% 
  mutate(top_hero_type = top_hero_type %>% str_to_title(), top_hero = top_hero %>% str_to_title()) %>%
  group_by(top_hero_type, top_hero) %>% 
  summarise(n = n()) %>% 
  ggplot(., aes(y=n, x=reorder(top_hero, n), fill=top_hero_type)) + 
  geom_bar(stat = "identity") +
  coord_flip() +
  theme_minimal() +
  labs(x='Hero', y='Number of players', title='Most popular heroes to play',
       fill = 'Hero type') +
  scale_fill_manual(values = c("#999999", "#E69F00", "#56B4E9"))
```

Reinhardt, a Tank, and Moira, a Support hero, are by far the two most popular choices in competitive Overwatch games on Playstation. Overall, Damage heroes are less often a player's most played character. That's because in competitive matches, strategy and special abilities like healing teammates are more important just damaging the opponents. For example, the most popular hero Reinhardt has the largest shield in the game which all of his other teammates can stand behind and be protected from oppoisition damage. This is really useful when trying to remain on the objective space and keeping the other team out.

Here we see that the `tank` and `support` type heroes are more popular than the `damage` heroes. Reinhardt was the most popular tank during the open beta and it appears his popularity remains strong. `Ana` is considered one of the most difficult heroes to play and master. The game developers may be interested to dig deeper to find out why `Pharah` is the least popular.

```{r, echo=FALSE}
# borrowed from week 8 HW
diagnostics <- function(model,
                        pcol = 'grey',
                        lcol = 'dodgerblue',
                        alpha = 0.05,
                        plotit = TRUE,
                        testit = TRUE
                        ){
  if (plotit){
    par(mfrow=c(1,2))
    # plot 1 - fitted vs resid
    plot(fitted(model), resid(model), col = pcol, pch = 20,
         xlab = "Fitted", ylab = "Residuals", main = "Residual versus fitted plot")
    abline(h = 0, col = lcol, lwd = 2)
    
    # plot 2
    qqnorm(resid(model), main = "Normal Q-Q Plot", col = pcol)
    qqline(resid(model), col = lcol, lwd = 2)
  }
  
  if (testit){
    st <- shapiro.test(resid(model))
    decision <- ifelse(st$p.value < alpha, 'Reject', 'Fail to Reject')
    return(list(p_val=st$p.value, decision=decision))
  }
}
```

##### Full additive model

We'll start to understand what predicts `skill_rating` by using all available predictors, including the most popular hero got a given player shown in the chart above.

We start with a basic additive model to get a sense of our data and to explore potential issues. Let's see if our assumptions of linear regression hold true (LINE).

```{r, cache=TRUE}
df %>% summary
fit_add_full = lm(skill_rating ~ . -top_hero_type, data = df)

diagnostics(fit_add_full, testit = FALSE)
```


```{r}
(fm_diag = diagnostics(fit_add_full, plotit = FALSE))
 # TO DO WRITE OUT TEST RESUTS OF SHAPIRO WILK
fm_bp = bptest(fit_add_full)
 # TO DO WRITE OUT TEST RESUTS OF BP
fm_bp
```

Our full additive model fails both the Shapiro-Wilk and Bruesch-Pagan test. We reject the null hypothesis in the Shapiro-Wilk test which means that there is only a small probability that the data could have been sampled from a normal distribution.

Also, we reject the null of homoscedasticity, which means that the constant variance assumption is violated. 

##### Full additive model problems

There are two major problems in the full additive model: heteroskedasticity and non-normal residuals. We can try to find the correct model and apply transformations to the predictors. Or, what we'll do instead is think more carefully about the predictors and hand-pick a smaller model to start with based on exploratory data analysis and our knowledge of Overwatch.

Let's look at the collinearity of our predictor variables.

```{r}
pair_vars = c("best.meleeFinalBlowsMostInGame",
              "best.offensiveAssistsMostInGame",
              "average.objectiveTimeAvgPer10Min",
              "average.allDamageDoneAvgPer10Min",
              "average.objectiveKillsAvgPer10Min",
              "games_played",
              "skill_rating")
pairs(df[pair_vars])
```

These pairwise plots look good. The `average.allDamageDoneAvgPer10Min` and `average.objectiveKillsAvgPer10Min` predictors appear to be slightly correlated but doesn't seem strong enough to remove one.

```{r}
find_cor_sr <- function(data){
  M <- cor(data %>% select_if(is.numeric))
  M[row.names(M) == 'skill_rating', !(colnames(M) %in% c('rank', 'skill_rating'))]
}

linear_cors = find_cor_sr(df)
sort(linear_cors, decreasing=TRUE) %>% head(5)
sort(linear_cors, decreasing=FALSE) %>% head(5)

cor(df$combat.meleeFinalBlows, df$best.meleeFinalBlowsMostInGame)

cor(df$average.allDamageDoneAvgPer10Min, df$average.barrierDamageDoneAvgPer10Min)
cor(df$average.allDamageDoneAvgPer10Min, df$best.offensiveAssistsMostInGame)
```

Explain `best.meleeFinalBlowsMostInGame` and why `combat.meleeFinalBlows` is redundant. wtih this.

Explain `average.allDamageDoneAvgPer10Min` and why it's more interpretable than `average.barrierDamageDoneAvgPer10Min` which is a result of your overall damage done per 10 minutes.

Explain `average.objectiveKillsAvgPer10Min` and why it's correlated with `average.objectiveTimeAvgPer10Min` but more actionable since it's more specific about what to do when a player is on the objective area.

And that's it. Those are the most correlated linear variables with SR we have.

- log(gp) - brian

##### Number of games played as a predictor

A player may need to get better to improve their "average" statistics laid out in the correlated variables above. But one thing *any* player can always do is play more. So we also want to consider the number of games played as a predictor of `skill_rating` as it's both actionable and an obvious variable to control for, i.e. are the most skilled just those who have played the most?

```{r}
cor(df$skill_rating, df$games_played)

par(mfrow=c(1,2))
plot(skill_rating ~ games_played, data = df)
plot(skill_rating ~ log(games_played), data = df)
```

We can see that the natural log transform of `games_played` makes the positive relationship with `skill_rating` easier to see and brings in the long tail of players who have played many more games than the median player. This will help prevent heteroskedasticity with this predictor in the linear model.

```{r, echo=FALSE}
df_final = df %>% 
  mutate(games_played = log(games_played))
```


Now, using the predictors we've identifed we run a backwards search starting with all of the predictors in the model.

```{r}
df_final = df %>% select(one_of(c(pair_vars, 'top_hero')))

fit = lm(skill_rating ~ top_hero : . + ., data = df_final)
summary(fit)
```

```{r include=FALSE}
# backward AIC
back_aic = step(fit, direction = "backward")
summary(back_aic)

# backward BIC
n = length(resid(fit))
back_bic = step(fit, direction = "backward", k = log(n))
summary(back_bic)
```

We wanted a smaller model for better explanatory power so we first tried a backwards search using BIC. However, the adjusted $R^2$ was lower than the model chosen using a backwards search using AIC.

```{r}
summary(back_aic)$adj.r.squared
summary(back_bic)$adj.r.squared
```

We see here that the model chosen by the AIC backwards search has more explanatory power.

```{r}
# null model
fit_null = lm(skill_rating ~ 1, data = df)
# model chosen by backwards BIC
fit_full = lm(skill_rating ~ best.meleeFinalBlowsMostInGame + 
    best.offensiveAssistsMostInGame + average.allDamageDoneAvgPer10Min + 
    average.objectiveKillsAvgPer10Min + games_played, 
    data = df)
anova(fit_null, fit_full)
```

Here we reject the null hypothesis and confirm that the model chosen by using a backwards search using BIC is significant.

```{r}
# compare BIC with AIC 
# model chosen by backwards BIC
fit_null = lm(skill_rating ~ best.meleeFinalBlowsMostInGame + 
    best.offensiveAssistsMostInGame + average.allDamageDoneAvgPer10Min + 
    average.objectiveKillsAvgPer10Min + games_played, 
    data = df)
# model chosen by backwards AIC
fit_full = lm(skill_rating ~ top_hero + best.meleeFinalBlowsMostInGame + 
    best.offensiveAssistsMostInGame + average.allDamageDoneAvgPer10Min + 
    average.objectiveKillsAvgPer10Min + games_played + top_hero:average.allDamageDoneAvgPer10Min, 
    data = df)
anova(fit_null, fit_full)
```

Again, we reject the null hypothesis and confirm that the addition of the interaction predictor `top_hero:average.allDamageDoneAvgPer10Min` to the model is significant (the model chosen by backwards AIC).


### Results


```{r}
fit_final = lm(formula = skill_rating ~ top_hero + best.meleeFinalBlowsMostInGame + 
    best.offensiveAssistsMostInGame + average.allDamageDoneAvgPer10Min + 
    average.objectiveKillsAvgPer10Min + games_played + top_hero:average.allDamageDoneAvgPer10Min, 
    data = df_final)

summary(fit_final)
diagnostics(fit_final, alpha = 0.01)

plot(fitted(fit_final), resid(fit_final))
hist(rstandard(fit_final))
bptest(fit_final)
```

We use 2 ANOVA tests to confirm that our final model is the best.

```{r}
# null model
fit_null = lm(skill_rating ~ 1, data = df)
# model chosen by backwards BIC
fit_full = lm(skill_rating ~ best.meleeFinalBlowsMostInGame + 
    best.offensiveAssistsMostInGame + average.allDamageDoneAvgPer10Min + 
    average.objectiveKillsAvgPer10Min + games_played, 
    data = df)
anova(fit_null, fit_full)
```

Here we reject the null hypothesis and confirm that the model chosen by using a backwards search using BIC is significant.

```{r}
# compare BIC with AIC 
# model chosen by backwards BIC
fit_null = lm(skill_rating ~ best.meleeFinalBlowsMostInGame + 
    best.offensiveAssistsMostInGame + average.allDamageDoneAvgPer10Min + 
    average.objectiveKillsAvgPer10Min + games_played, 
    data = df)
# model chosen by backwards AIC
fit_full = lm(skill_rating ~ top_hero + best.meleeFinalBlowsMostInGame + 
    best.offensiveAssistsMostInGame + average.allDamageDoneAvgPer10Min + 
    average.objectiveKillsAvgPer10Min + games_played + top_hero:average.allDamageDoneAvgPer10Min, 
    data = df)
anova(fit_null, fit_full)
```

Again, we reject the null hypothesis and confirm that the addition of the interaction predictor `top_hero:average.allDamageDoneAvgPer10Min` to the model is significant (the model chosen by backwards AIC).


### Discussion

So, what should a player who wants to improve their skill rating focus on?

- coef plot - brian

```{r}
library(coefplot)
library(dotwhisker)
library(broom)

sort(coef(fit_final), decreasing = TRUE) %>% head(5)
sort(coef(fit_final), decreasing = FALSE) %>% head(5)
fit_coefs = coef(fit_final)
fit_coefs_names = names(fit_coefs)
first_order_th = grepl('top_hero', fit_coefs_names) & !(grepl('DoneAvgPer10Min', fit_coefs_names))
fit_coefs_names[first_order_th]

coefplot(fit_final, predictors=c('top_hero'))

m1 = tidy(fit_final)

m1 %>% 
  filter(grepl('top_hero', term) & !(grepl('DoneAvgPer10Min', term))) %>% 
  mutate(term = str_replace(term, 'top_hero', '') %>% str_to_title()) %>% 
  dwplot(vline = geom_vline(xintercept = 0, colour = "grey60", linetype = 2)) +
  theme_bw() +
  theme(legend.title = element_blank()) +
  guides(shape='none', colour='none') + 
  labs(x='Coefficients and confidence intervals', caption = 'Omitted hero is Reinhardt') + 
  ggtitle('Effect of Choosing Certain Heroes on Skill Rating')

```

It's pretty clear that playing Ana, Mercy and Moira are associated with higher skill rating. These three heroes are all support heroes which suggests learning how to be a good support hero on your team is a good way to boost your skill rating.

For example ... INTERPRET ANA's dummy variable here

- coefficients interpretation - brian, kai

Looking at the coefficients for our interaction predictor `top_hero:average.allDamageDoneAvgPer10Min` we see that `top_heroana:average.allDamageDoneAvgPer10Min` and `top_heromercy:average.allDamageDoneAvgPer10Min` are both significant at $\alpha = 0.01$. 

We see that the coefficients for `top_heroana:average.allDamageDoneAvgPer10Min`: ``r coef(back_aic)["top_heroana:average.allDamageDoneAvgPer10Min"]`` and `top_heromercy:average.allDamageDoneAvgPer10Min`: ``r coef(back_aic)["top_heromercy:average.allDamageDoneAvgPer10Min"]`` are both negative. This tells us that doing damage as a support character does not boost your skill rating as much as if you were doing damange as a tank. 

We can see this by comparing our coefficients: 

- `top_heroana:average.allDamageDoneAvgPer10Min` + `allDamageDoneAvgPer10Min` = ``r coef(back_aic)["top_heroana:average.allDamageDoneAvgPer10Min"] + coef(back_aic)["average.allDamageDoneAvgPer10Min"]``
- `average.allDamageDoneAvgPer10Min` = ``r coef(back_aic)["average.allDamageDoneAvgPer10Min"]``

Because the `top_hero` predictor is releveled with `Reinhardt` (a tank) as the base, we can interpret the above values as such: when playing as a tank your skill rating benefits more by doing a lot of damage than if you were doing a lot of damage as a support character (Ana & Mercy). 

Now, looking at `games_played`: ``r coef(back_aic)["games_played"]``, we can see that your skill rating also increases with the number of games you play. Rescaling the coefficient back to it's orignal units $e^\beta$ = ``r exp(coef(back_aic)["games_played"])`` we see that your average skill rating goes up by ~`r exp(coef(back_aic)["games_played"])` points per game played. This makes sense because it is fair to assume that you improve as a player the more you play. Practice always helps!

### Appendix
