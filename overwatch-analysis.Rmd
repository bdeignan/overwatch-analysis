---
title: "Data Analysis Project - How to Increase your Overwatch Skill Rating"
author: "Brian Deignan (deignan3) & Kai Chang (kaihc2)"
date: 'August 3, 2019'
output:
  html_document:
    theme: flatly 
    toc: yes
    fig_width: 10
  pdf_document: default
urlcolor: cyan
---

![Overwatch heroes Lucio, Wrecking Ball and Soldier 76 in action](ow6.jpg)

***

## Introduction
Our dataset consists of information on competitive gamers who play the video game Overwatch on Playstation 4. [Overwatch](https://en.wikipedia.org/wiki/Overwatch_(video_game)) is a team-based multiplayer first-person shooter developed and published by Blizzard Entertainment. Overwatch assigns players into two teams of six, with each player selecting from a roster of 30 characters, known as "heroes", each with a unique style of play whose roles are divided into three general categories that fit their role. Players on a team work together to secure and defend control points on a map or escort a payload across the map in a limited amount of time. 

Overwatch has a large community and E-Sports presence online. Players' skill in competitive games is calculated by a "secret" formula at Blizzard that leads to a "skill rating", or "SR" for short. SR ranges from 0 to 5,000, the higher the score the better the player.

Among the community, SR is divided into categories depending on how high the rating is, ranging from Bronze to Grandmaster:

- 500-1499 Bronze
- 1500-1999 Silver
- 2000-2499 Gold
- 2500-2999 Platinum
- 3000-3499 Diamond
- 3500-3999 Master
- 4000-5000 Grandmaster

#### Gameplay

Overwatch pairs two teams up against each other in a match, or game. Each team has six members who must each choose a unique character, called a "hero". More on that later. The goal is to damage the other team and help keep your team protected. In addition to damaging the other team's players, the team has an "objective" to complete, which varies from map to map. A "map" here refers to a setting where a match takes place. The objective in Overwatch typically depends on members from one team being located in a specific area, refered to as the objective, while the other team isn't in that same area. While in the objective area, your team accrues time spent on the objective. If your team accrues enough time, you win the match. It's a little more complicated than this but that's essentially it. 

Holding down an objecitve area is where damaging your opponent comes in to play. When the other team tries to get on the objective at the same time as your team, you deal them damage to keep them away. Hopefully do enough that the opponent "dies". But don't worry, Overwatch is what's called a "respawn" game. After a player "dies" from taking too much damage, they must wait a few seconds and then they rejoin the game and try again to take the objective from the other team.

#### Data

We scraped a snapshot of PS4 players' SR (it changes from game to game) whose profiles were public on overwatchtracker.com. We then scraped players' career statistics from the games that they've played from the open source API ovrstat.com which returns convenient JSON formatted data.

Currently we have over two thousand player skill ratings and over two thousand predictor variables. However, our research question will allow us to tailor our question to a small subset of the predictors, around 60 or so.

#### Research Question

Overall we're interested in the question: if a player wants to improve their SR, what should they focus on? Should they try to eliminate more opponents? Heal their teammates? Or, play a certain character? Answers like these will be provided by a predictive model of SR using career player statistics as predictors. The answers we find will allow any player to most efficiently improve their SR and begin climbing their way to Grandmaster!

The answers we find could be used by amateurs and pro Overwatch gamers alike. We think of our analysis as the start of something like ["Moneyball"](https://en.wikipedia.org/wiki/Moneyball) for Overwatch.

## Methods
```{r, echo=FALSE, message=FALSE}
library(tidyverse)

# r = getOption("repos")
# r["CRAN"] = "http://cran.us.r-project.org"
# options(repos = r)

# putting necessary libraries here too
if (!require(coefplot)) {
    install.packages("coefplot")
    require(coefplot)
}
# putting necessary libraries here too
if (!require(lmtest)) {
    install.packages("coefplot")
    require(lmtest)
}


```

```{r, message=FALSE}
df = read_csv('data/clean-data.csv')
```

Here we releved our factor variable to use the character `Reinhardt` as the reference level. Reinhardt is the most popular hero used in the game and would be a good baseline to use for comparison.

```{r, echo=FALSE, message=FALSE}
# relevel top hero - brian
df = df %>% 
  select(-c(game.gamesPlayed, game.timePlayed, rank)) %>% 
  mutate(top_hero = relevel(top_hero %>% as.factor(), ref='reinhardt'))
```

Let's explore the data a bit more...

```{r}
df %>% glimpse()
df %>% summary()
```


The main question we want to tackle is: what explains the variation in `skill_rating`? Let's first look at the distribution of `skill_rating`.

```{r, echo=FALSE, cache=TRUE}
mean_sr = mean(df$skill_rating)
sd_sr = sd(df$skill_rating)

hist(df$skill_rating,
     prob = TRUE,
     xlab   = "Skill Rating",
     main   = "Histogram of Skill Rating (Playstation 4 players)",
     breaks = 20,
     col    = "#56B4E9",
     border = "#999999")
curve(dnorm(x, mean = mean_sr, sd = sd_sr), 
      col = "#E69F00", add = TRUE, lwd = 3)
```

Above is a normalized (to sum to 1) histogram of the reponse we want to model `skill_rating`. It appears that `skill_rating` (blue bars) looks a lot like a normal distribution (the orange line). This is a good thing, as this makes it easier to adhere to the assumptions of linear regression model we will use to explain the variation in player skill.

#### Most played heroes

At the time of the analysis, there are 30 heroes in Overwatch to choose from. Each hero has a unique play style. There are three types of heroes which determine the play style: **Tank**, **Damage**, or **Support**. **Tank** heroes can take a lot of damage and typically protect their teammates while trying to remain on the objective area. **Damage** heroes have the ability to do the most damage to the opposition. They have lower health than Tank heroes, meaning they can "die" more easily. 

Lastly, there are **Support** heroes. These heroes typically have the least health and do the least amount of damage to opponents. Instead of holding down an objective or damaging opponents, Support heroes main job to restore the health of their teammates, which is their primary special role on the team. That being said, all heroes can do damage and a really good Support player can beat a hard-to-kill tank in the game. You can read more about the heroes [here](https://en.wikipedia.org/wiki/Characters_of_Overwatch#Characters).

First, let's see which heroes are the most popular to play, and which type of hero they are:

```{r, echo=FALSE, cache=TRUE}
# - bar chart of top players - brian
df %>% 
  mutate(top_hero_type = top_hero_type %>% str_to_title(), top_hero = top_hero %>% str_to_title()) %>%
  group_by(top_hero_type, top_hero) %>% 
  summarise(n = n()) %>% 
  ggplot(., aes(y=n, x=reorder(top_hero, n), fill=top_hero_type)) + 
  geom_bar(stat = "identity") +
  coord_flip() +
  theme_minimal() +
  labs(x='Hero', y='Number of players', title='Most popular heroes to play',
       fill = 'Hero type') +
  scale_fill_manual(values = c("#999999", "#E69F00", "#56B4E9"))
```

Reinhardt, a Tank, and Moira, a Support hero, are by far the two most popular choices in competitive Overwatch games on Playstation. Overall, Damage heroes are less often a player's most-played character according the career stats data above. One reason for this is that, in competitive matches, strategy and special abilities like healing teammates are more important simply damaging your opponents. For example, the most popular hero Reinhardt has the largest shield in the game which all of his other teammates can stand behind and be protected from opposition damage. This is really useful when trying to remain on the objective area and keeping the other team out.

Here we see that the `Tank` and `Support` type heroes are more popular than the `Damage` heroes. Reinhardt was the most popular tank during the open beta and it appears his popularity remains strong. `Ana` is considered one of the most difficult heroes to play and master. The game developers may be interested to dig deeper to find out why `Pharah` is the least popular. Perhaps it's because she's an easy target for skilled players to hit, has only an average amount of health and cannot heal herself.

```{r, echo=FALSE}
# borrowed from week 8 HW
diagnostics <- function(model,
                        pcol = 'grey',
                        lcol = 'dodgerblue',
                        alpha = 0.05,
                        plotit = TRUE,
                        testit = TRUE
                        ){
  if (plotit){
    par(mfrow=c(1,2))
    # plot 1 - fitted vs resid
    plot(fitted(model), resid(model), col = pcol, pch = 20,
         xlab = "Fitted", ylab = "Residuals", main = "Residual versus fitted plot")
    abline(h = 0, col = lcol, lwd = 2)
    
    # plot 2
    qqnorm(resid(model), main = "Normal Q-Q Plot", col = pcol)
    qqline(resid(model), col = lcol, lwd = 2)
  }
  
  if (testit){
    st <- shapiro.test(resid(model))
    decision <- ifelse(st$p.value < alpha, 'Reject', 'Fail to Reject')
    return(list(p_val=st$p.value, decision=decision))
  }
}
```

#### Full additive model

We'll start to understand what predicts `skill_rating` by using all available predictors, including the most popular hero got a given player shown in the chart above.

We start with a basic additive model to get a sense of our data and to explore potential issues. Let's see if our assumptions of linear regression hold true (LINE).

```{r, cache=TRUE}
fit_add_full = lm(skill_rating ~ . -top_hero_type, data = df)

diagnostics(fit_add_full, testit = FALSE)
```


```{r}
(fm_diag = diagnostics(fit_add_full, plotit = FALSE))
fm_bp = bptest(fit_add_full)
fm_bp
```

Our full additive model fails both the Shapiro-Wilk and Bruesch-Pagan test. We reject the null hypothesis in the Shapiro-Wilk test which means that there is only a small probability that the data could have been sampled from a normal distribution.

Also, we reject the null of homoscedasticity, which means that the constant variance assumption is violated. 

#### Full additive model problems

There are two major problems in the full additive model: heteroskedasticity and non-normal residuals. We can try to find the correct model and apply transformations to the predictors. Or, what we'll do instead is think more carefully about the predictors and hand-pick a smaller model to start with based on exploratory data analysis and our knowledge of Overwatch.

Let's look at the collinearity of our predictor variables.

```{r, fig.height=10}
pair_vars = c("best.meleeFinalBlowsMostInGame",
              "best.offensiveAssistsMostInGame",
              "average.objectiveTimeAvgPer10Min",
              "average.allDamageDoneAvgPer10Min",
              "average.objectiveKillsAvgPer10Min",
              "games_played",
              "skill_rating")
pairs(df[pair_vars])
```

These pairwise plots look good. The `average.allDamageDoneAvgPer10Min` and `average.objectiveKillsAvgPer10Min` predictors appear to be slightly correlated but doesn't seem strong enough to remove one.

```{r}
find_cor_sr <- function(data){
  M <- cor(data %>% select_if(is.numeric))
  M[row.names(M) == 'skill_rating', !(colnames(M) %in% c('rank', 'skill_rating'))]
}

linear_cors = find_cor_sr(df)
```

The five most positively correlated variables with `skill_rating` are: ``r sort(linear_cors, decreasing=TRUE) %>% head(5) %>% names()``.

Among the most positively correlated variables there are some redundant variables. For example, `best.meleeFinalBlowsMostInGame` and `combat.meleeFinalBlows` are both measures of how many melees a player does in a game that "killed" an opponent hero. They are highly correlated, with a correlation coefficient of `r cor(df$combat.meleeFinalBlows, df$best.meleeFinalBlowsMostInGame)`. So we only need to keep the more correlated variable in our next model.

Another conceptually redundant pair of variables is `average.allDamageDoneAvgPer10Min` and `average.barrierDamageDoneAvgPer10Min`. `average.allDamageDoneAvgPer10Min` is more interpretable than `average.barrierDamageDoneAvgPer10Min`. That's because when you average a lot of damage output per 10 minutes, and there happens to be a barrier you're damaging (for example, Reinhardt's shield), then `average.barrierDamageDoneAvgPer10Min` is just a side effect of damage output. And this is seen in the data, these two variables are highly correlated, with a corrleation of `r cor(df$average.allDamageDoneAvgPer10Min, df$average.barrierDamageDoneAvgPer10Min)`.

The two most negatively correlated variables with `skill_rating` are: ``r sort(linear_cors, decreasing=FALSE) %>% head(2) %>% names()``.

Again, these two variables are conceptually very similar so we only need to keep one in the next model: `average.objectiveKillsAvgPer10Min`. `average.objectiveKillsAvgPer10Min` is more actionable than `average.objectiveTimeAvgPer10Min` since it's more specific about what to do when a player is on the objective area, get "kills".

#### Number of games played as a predictor

A player may need to get better to improve their "average" statistics laid out in the correlated variables above. But one thing *any* player can always do is play more. So we also want to consider the number of games played as a predictor of `skill_rating` as it's both actionable and an obvious variable to control for, i.e. are the most skilled just those who have played the most?

```{r}
par(mfrow=c(1,2))
plot(skill_rating ~ games_played, data = df)
plot(skill_rating ~ log(games_played), data = df)
```

We can see that the natural log transform of `games_played` makes the positive relationship with `skill_rating` easier to see and brings in the long tail of players who have played many more games than the median player. This will help prevent heteroskedasticity with this predictor in the linear model. The correlation between `games_played` and `skill_rating` is `r cor(df$skill_rating, log(df$games_played))`.

```{r, echo=FALSE}
df_final = df %>% 
  mutate(games_played = log(games_played))
```


Now, using the predictors we've identifed we run a backwards search starting with all of the predictors in the model.

```{r}
df_final = df %>% select(one_of(c(pair_vars, 'top_hero')))

fit = lm(skill_rating ~ top_hero : . + ., data = df_final)
summary(fit)
```

```{r include=FALSE}
# backward AIC
back_aic = step(fit, direction = "backward")
summary(back_aic)

# backward BIC
n = length(resid(fit))
back_bic = step(fit, direction = "backward", k = log(n))
summary(back_bic)
```

We wanted a smaller model for better explanatory power so we first tried a backwards search using BIC. However, the adjusted $R^2$ was lower than the model chosen using a backwards search using AIC.

```{r}
summary(back_aic)$adj.r.squared
summary(back_bic)$adj.r.squared
```

We see here that the model chosen by the AIC backwards search has more explanatory power.

```{r}
# null model
fit_null = lm(skill_rating ~ 1, data = df)
# model chosen by backwards BIC
fit_full = lm(skill_rating ~ best.meleeFinalBlowsMostInGame + 
    best.offensiveAssistsMostInGame + average.allDamageDoneAvgPer10Min + 
    average.objectiveKillsAvgPer10Min + games_played, 
    data = df)
anova(fit_null, fit_full)
```

Here we reject the null hypothesis and confirm that the model chosen by using a backwards search using BIC is significant.

```{r}
# compare BIC with AIC 
# model chosen by backwards BIC
fit_null = lm(skill_rating ~ best.meleeFinalBlowsMostInGame + 
    best.offensiveAssistsMostInGame + average.allDamageDoneAvgPer10Min + 
    average.objectiveKillsAvgPer10Min + games_played, 
    data = df)
# model chosen by backwards AIC
fit_full = lm(skill_rating ~ top_hero + best.meleeFinalBlowsMostInGame + 
    best.offensiveAssistsMostInGame + average.allDamageDoneAvgPer10Min + 
    average.objectiveKillsAvgPer10Min + games_played + top_hero:average.allDamageDoneAvgPer10Min, 
    data = df)

anova(fit_null, fit_full)
```

Again, we reject the null hypothesis and confirm that the addition of the interaction predictor `top_hero:average.allDamageDoneAvgPer10Min` to the model is significant (the model chosen by backwards AIC).


## Results

We use 2 ANOVA tests to confirm that our final model is the best.

```{r}
# null model
fit_null = lm(skill_rating ~ 1, data = df)
# model chosen by backwards BIC
fit_full = lm(skill_rating ~ best.meleeFinalBlowsMostInGame + 
    best.offensiveAssistsMostInGame + average.allDamageDoneAvgPer10Min + 
    average.objectiveKillsAvgPer10Min + games_played, 
    data = df)
anova(fit_null, fit_full)
summary(fit_full)$adj.r.squared
```

Here we reject the null hypothesis and confirm that the model chosen by using a backwards search using BIC is significant.

```{r}
# compare BIC with AIC 
# model chosen by backwards BIC
fit_null = lm(skill_rating ~ best.meleeFinalBlowsMostInGame + 
    best.offensiveAssistsMostInGame + average.allDamageDoneAvgPer10Min + 
    average.objectiveKillsAvgPer10Min + games_played, 
    data = df)
# model chosen by backwards AIC
fit_full = lm(skill_rating ~ top_hero + best.meleeFinalBlowsMostInGame + 
    best.offensiveAssistsMostInGame + average.allDamageDoneAvgPer10Min + 
    average.objectiveKillsAvgPer10Min + games_played + top_hero:average.allDamageDoneAvgPer10Min, 
    data = df)
anova(fit_null, fit_full)
summary(fit_full)$adj.r.squared
```

Again, we reject the null hypothesis and confirm that the addition of the interaction predictor `top_hero:average.allDamageDoneAvgPer10Min` to the model is significant (the model chosen by backwards AIC).

The model chosen by AIC has a higher $R^2$ than the model chosen using BIC. Therefore, our best model is the one chosen by AIC. We'll now check the assumptions of the model to make sure we can trust the hypothesis tests since we're interested in finding out what variables best explain `skill_rating` and how they affect `skill_rating`.

```{r}
fit_final = lm(formula = skill_rating ~ top_hero + best.meleeFinalBlowsMostInGame + 
    best.offensiveAssistsMostInGame + average.allDamageDoneAvgPer10Min + 
    average.objectiveKillsAvgPer10Min + games_played + top_hero:average.allDamageDoneAvgPer10Min, 
    data = df_final)

summary(fit_final)
diagnostics(fit_final, alpha = 0.01, testit = FALSE)
par(mfrow=c(1,1))
hist(rstandard(fit_final), main='Histogram of standardized residuals')
```

```{r}
sw_final = diagnostics(fit_final, alpha = 0.01, plotit = FALSE)
bp_final = bptest(fit_final)
```

At the level $\alpha = 0.05$, we fail to reject the $H_0$ of the Shapiro-Wilk test and find that the residuals don't violate the normality assumption, with a $p-value$ of `r sw_final$p_val`.

Similarly, at the level $\alpha = 0.05$, we find evidence that the errors are homoskedastic, failing to reject $H_0$ for the Breusch-Pagan test, with a $p-value$ of `r bp_final$p.value`.

## Discussion

So, what should a player who wants to improve their skill rating focus on? We look to our best model to help us.

```{r}
library(coefplot)
library(dotwhisker)
library(broom)

m1 = tidy(fit_final)

m1 %>% 
  filter(grepl('top_hero', term) & !(grepl('DoneAvgPer10Min', term))) %>% 
  mutate(term = str_replace(term, 'top_hero', '') %>% str_to_title()) %>% 
  dwplot(vline = geom_vline(xintercept = 0, colour = "grey60", linetype = 2)) +
  theme_bw() +
  theme(legend.title = element_blank()) +
  guides(shape='none', colour='none') + 
  labs(x='Coefficients and confidence intervals', caption = 'Omitted hero is Reinhardt') + 
  ggtitle('Effect of Choosing Certain Heroes on Skill Rating')

```

It's pretty clear that playing Ana, Mercy, Lucio and Moira are associated with higher skill rating. These four heroes are all support heroes which suggests learning how to be a good support hero on your team is a good way to boost your skill rating.

For example, for players who "main" Ana rather than Reinhardt, that is those who play Ana the most, they have a higher `skill_rating`. Ana players on average have a `r coef(back_aic)["top_heroana"]` higher `skill_rating` than players who "main" Reinhardt.

So, if you want to get a higher `skill_rating`, you should consider playing one the four Support heroes: Ana, Lucio, Mercy or Moira.

Looking at the coefficients for our interaction predictor `top_hero:average.allDamageDoneAvgPer10Min` we see that `top_heroana:average.allDamageDoneAvgPer10Min` and `top_heromercy:average.allDamageDoneAvgPer10Min` are both significant at $\alpha = 0.01$. 

We see that the coefficients for `top_heroana:average.allDamageDoneAvgPer10Min`: ``r coef(back_aic)["top_heroana:average.allDamageDoneAvgPer10Min"]`` and `top_heromercy:average.allDamageDoneAvgPer10Min`: ``r coef(back_aic)["top_heromercy:average.allDamageDoneAvgPer10Min"]`` are both negative. This tells us that doing damage as a support character does not boost your skill rating as much as if you were doing damange as a tank. 

We can see this by comparing our coefficients: 

- `top_heroana:average.allDamageDoneAvgPer10Min` + `allDamageDoneAvgPer10Min` = ``r coef(back_aic)["top_heroana:average.allDamageDoneAvgPer10Min"] + coef(back_aic)["average.allDamageDoneAvgPer10Min"]``
- `average.allDamageDoneAvgPer10Min` = ``r coef(back_aic)["average.allDamageDoneAvgPer10Min"]``

Because the `top_hero` predictor is releveled with `Reinhardt` (a tank) as the base, we can interpret the above values as such: when playing Reinhardt, the most popular hero, your skill rating benefits more by doing a lot of damage than if you were doing a lot of damage as a Support character (Ana & Mercy). 

Now, looking at `games_played`: ``r coef(back_aic)["games_played"]``, we can see that your skill rating also increases with the number of games you play. Rescaling the coefficient back to it's orignal units $e^\beta$ = ``r exp(coef(back_aic)["games_played"])`` we see that your average skill rating goes up by ~`r exp(coef(back_aic)["games_played"])` points per game played. This makes sense because it is fair to assume that you improve as a player the more you play. Practice always helps!

#### Summary of findings

Our best model shows that if you want to climb the Overwatch skill leaderboard on Playstation 4, you should:

- Play Support heroes
- If you play a Tank, do a lot of damage to opponents, don't just stand there and protect your teammates. Go on the offensive!
- Play more games! This is obvious but important to remember. Sometimes you will feel you hit a skill wall and cannot improve. Our model however provides evidence that you should keep going! Play more and your `skill_rating` should improve.

The next step that could improve this analysis would be to track players' `skill_rating` over time and track what changes in variables lead to changes in `skill_rating`. However, for a first analysis of this data by anyone (to the best of our knowledge), we were able to get a good explanatory model of skill in Overwatch to help players focus their efforts and climb the rankings.

